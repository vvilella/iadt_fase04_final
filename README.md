Tech Challenge â€“ IADT â€“ Fase 4
AnÃ¡lise Inteligente de VÃ­deo com IA

Este projeto implementa uma aplicaÃ§Ã£o de anÃ¡lise automÃ¡tica de vÃ­deo, com foco em visÃ£o computacional e inteligÃªncia artificial, conforme os requisitos da Fase 4 da disciplina InteligÃªncia Artificial para Devs (IADT).

A aplicaÃ§Ã£o processa um vÃ­deo fixo fornecido pela plataforma, gera um vÃ­deo anotado e um relatÃ³rio estruturado em JSON, contendo mÃ©tricas e inferÃªncias realizadas ao longo do processamento.

ğŸ¯ Objetivo do Projeto

Construir um pipeline de anÃ¡lise de vÃ­deo capaz de:

Processar um vÃ­deo frame a frame

Detectar rostos

Analisar expressÃµes emocionais

Gerar anotaÃ§Ãµes visuais no vÃ­deo

Produzir um relatÃ³rio final consolidado

A arquitetura foi pensada de forma modular, permitindo a evoluÃ§Ã£o incremental do projeto.

ğŸ§± Arquitetura Geral

O pipeline atual Ã© composto por:

Leitura e escrita de vÃ­deo (OpenCV)

Loop de processamento de frames com callback (on_frame)

Contexto de anÃ¡lise para acumular mÃ©tricas globais

MÃ³dulos independentes para cada tipo de anÃ¡lise (faces, emoÃ§Ãµes, etc.)

RelatÃ³rio final em JSON

video -> frames -> anÃ¡lises -> vÃ­deo anotado + report.json

ğŸ“ Estrutura do Projeto
iadt_fase04_final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Pipeline principal
â”‚   â”œâ”€â”€ frame_loop.py           # Loop genÃ©rico de frames
â”‚   â”œâ”€â”€ io_video.py             # Abertura e escrita de vÃ­deo
â”‚   â”œâ”€â”€ report.py               # GeraÃ§Ã£o do relatÃ³rio final
â”‚   â”œâ”€â”€ context.py              # Contexto global de mÃ©tricas
â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â””â”€â”€ face_detector.py    # DetecÃ§Ã£o facial (MediaPipe)
â”‚   â””â”€â”€ analyzers/
â”‚       â””â”€â”€ emotion_analyzer_openai.py  # EmoÃ§Ãµes via OpenAI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_video.mp4        # VÃ­deo de entrada (ignorado no git)
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ annotated.mp4           # VÃ­deo anotado
â”‚   â””â”€â”€ report.json             # RelatÃ³rio final
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âœ… Funcionalidades Implementadas atÃ© o Momento
ğŸ”¹ R1 â€“ VÃ­deo de Entrada Fixo

O vÃ­deo Ã© fornecido como entrada fixa conforme o enunciado.

O pipeline assume um Ãºnico vÃ­deo de anÃ¡lise por execuÃ§Ã£o.

ğŸ”¹ R2 â€“ Reconhecimento Facial

Implementado com MediaPipe Face Detection (execuÃ§Ã£o local).

Para cada frame:

Detecta rostos

Desenha bounding boxes no vÃ­deo

MÃ©tricas coletadas:

Total de frames analisados

Frames com pelo menos um rosto

Total de detecÃ§Ãµes faciais

Essas informaÃ§Ãµes sÃ£o incluÃ­das no relatÃ³rio final (report.json).

ğŸ”¹ R3 â€“ AnÃ¡lise de EmoÃ§Ãµes (Abordagem HÃ­brida)
Abordagem Atual (Implementada)

DetecÃ§Ã£o facial local (MediaPipe)

ClassificaÃ§Ã£o de emoÃ§Ãµes via OpenAI Vision API

EmoÃ§Ã£o analisada por amostragem temporal (a cada N frames)

A Ãºltima emoÃ§Ã£o vÃ¡lida Ã© mantida como cache para exibiÃ§Ã£o contÃ­nua no vÃ­deo

EmoÃ§Ãµes consideradas:

neutral

happy

sad

angry

fear

surprise

disgust

O resultado Ã©:

Exibido no vÃ­deo anotado

Agregado no relatÃ³rio final como contagem de ocorrÃªncias

ğŸ§ª Tentativa Inicial: EmoÃ§Ãµes 100% Locais (NÃ£o Utilizada)

Inicialmente, foi avaliada uma abordagem totalmente local para anÃ¡lise de emoÃ§Ãµes utilizando a biblioteca DeepFace (baseada em TensorFlow/Keras).

No entanto, durante a integraÃ§Ã£o com o MediaPipe, surgiram conflitos de dependÃªncias, especialmente envolvendo:

TensorFlow / Keras

VersÃµes incompatÃ­veis de protobuf

Esses conflitos inviabilizaram a execuÃ§Ã£o estÃ¡vel de ambos os componentes no mesmo ambiente Python.

DecisÃ£o Arquitetural

Optou-se por uma abordagem hÃ­brida, que:

MantÃ©m a detecÃ§Ã£o facial local

Remove dependÃªncias pesadas (TensorFlow/Keras)

Garante estabilidade do ambiente

Preserva a modularidade do pipeline

Essa decisÃ£o foi tomada visando robustez, clareza e prazo, sem comprometer os objetivos do trabalho.

ğŸ“Š Exemplo de RelatÃ³rio Gerado
{
  "total_frames_analyzed": 3326,
  "frames_with_face_detected": 2033,
  "total_face_detections": 2449,
  "emotions": {
    "neutral": 19,
    "happy": 7,
    "surprise": 5,
    "sad": 2,
    "disgust": 1,
    "fear": 1
  },
  "anomalies_count": 0
}

âš™ï¸ Requisitos e ExecuÃ§Ã£o
DependÃªncias principais

Python 3.11

OpenCV

MediaPipe

OpenAI SDK

ExecuÃ§Ã£o
python src/main.py --video data/sample_video.mp4


âš ï¸ Para R3, Ã© necessÃ¡rio definir a variÃ¡vel de ambiente OPENAI_API_KEY.

ğŸ§© LimitaÃ§Ãµes Conhecidas e Trabalhos Futuros

A anÃ¡lise de emoÃ§Ãµes depende atualmente de uma API externa (OpenAI).

Como melhoria futura (bÃ´nus), o mÃ³dulo de emoÃ§Ãµes pode ser substituÃ­do por um classificador local baseado em CNNs prÃ©-treinadas (ex: FER2013 ou AffectNet).

A arquitetura foi projetada para permitir essa substituiÃ§Ã£o sem impacto nos demais mÃ³dulos.

ğŸš§ PrÃ³ximos Passos Planejados

R4 â€“ DetecÃ§Ã£o e categorizaÃ§Ã£o de atividades

R5 â€“ DetecÃ§Ã£o de anomalias

R6 â€“ GeraÃ§Ã£o de resumo automÃ¡tico do vÃ­deo

Essas funcionalidades serÃ£o implementadas de forma incremental, com commits separados.

âœï¸ Autor

Victor Nardi Vilella
PÃ³s-GraduaÃ§Ã£o â€“ IA para Devs (FIAP)